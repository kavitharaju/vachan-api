{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "resident-attitude",
   "metadata": {},
   "source": [
    "### Some Previous Discussions\n",
    "\n",
    "[Points noted down to be improved in previous AgMT APIs](https://teams.microsoft.com/l/file/A8BD6EC4-4946-482C-A7C0-5DFA0AC96ACA?tenantId=dc5352cb-2fb1-4f19-a355-61b4398ec2e1&fileType=docx&objectUrl=https%3A%2F%2Fbridgeconn.sharepoint.com%2Fsites%2FDevTeam-AgMT-VachanAPI%2FShared%20Documents%2FAgMT%20-%20VachanAPI%2FAPI%20Refactoring%2FAgMT%20API%20Revision.docx&baseUrl=https%3A%2F%2Fbridgeconn.sharepoint.com%2Fsites%2FDevTeam-AgMT-VachanAPI&serviceName=teams&threadId=19:eafa29b748664314b67c8a8105d7caec@thread.tacv2&groupId=0d8df138-370a-4ec7-917d-8ec0699577f6)\n",
    "\n",
    "[The discussion document on suggestions module](https://teams.microsoft.com/l/file/44E2A83F-30FC-49CA-8E74-5BDDF1824DFC?tenantId=dc5352cb-2fb1-4f19-a355-61b4398ec2e1&fileType=docx&objectUrl=https%3A%2F%2Fbridgeconn.sharepoint.com%2Fsites%2FDevTeam-AgMT-VachanAPI%2FShared%20Documents%2FAgMT%20-%20VachanAPI%2FAPI%20Refactoring%2FSuggestions%20Module.docx&baseUrl=https%3A%2F%2Fbridgeconn.sharepoint.com%2Fsites%2FDevTeam-AgMT-VachanAPI&serviceName=teams&threadId=19:eafa29b748664314b67c8a8105d7caec@thread.tacv2&groupId=0d8df138-370a-4ec7-917d-8ec0699577f6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-warehouse",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "How about we use single word tokens for now(beta release in June)?\n",
    "\n",
    "*Answer*: No, better use phrases now itself or the design choices we make now would make it not possible to upgrade later\n",
    "\n",
    "Issues in using phrases\n",
    "1. The best way to capture alignments is to use single word tokens. As we plan to use alignments to automaticaly identify token translation and enrich translation memory, using single word tokens would be easier to begin with. As we are planning to give context based suggestions, phrases of at least 3 words would always be considered in effect. So we may still get the quality improvement of using phrase tokens instead of single word.\n",
    "2. Using phrase tokens increase the number of tokens to be translated considerably. Translators doesn't seem very happy about it. Now that context based translations are also going to be encouraged, they may feel their work is too much in token translation phase.\n",
    "\n",
    "Other changes in tokenization\n",
    "* The fucntion below takes **any list of sentences** as input(as list of (id, sentence) tuples) and tokenize them into single word tokens. This gives us flexibility to do tokenization in any desired manner: whole bible at once, some books, some chapters of a book etc. Later, in Autographa or outside, the same function can be used to translate other text contents like commenatries, notes or stories.\n",
    "* The tokens are returned in chronological order, that is **in the order they appear in the input text**. I hope this will give the user a better connection to the source while translating and also a better idea about the progress he is making through the source text.\n",
    "* The tokens are returned as a dict/json and the value of each token-key would be **the list of occurances** of the token. This would become handy for UI app for highlighlighting occurances and also returning the occurances where each sense is to be applied back to server. The server can also directly save the offset returned by UI and use it while draft generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "secure-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import utils\n",
    "\n",
    "def tokenize(sent_list, punctuations=utils.punctuations()+utils.numbers()):\n",
    "\t'''Get single word tokens and their occurances from input sentence list\n",
    "\tinput: [(sent_id, sent_text), (sent_id, sent_text), ...]\n",
    "\toutput: {\"token\": [(sent_id, offset),(sent_id, offset)..],\n",
    "\t         \"token\": [(sent_id, offset),(sent_id, offset)..], ...}'''\n",
    "\tunique_tokens = {}\n",
    "\tfor sent in sent_list:\n",
    "\t\tclean_sent = sent[1]\n",
    "\t\tfor punct in punctuations:\n",
    "\t\t\tclean_sent = clean_sent.replace(punct, \" \")\n",
    "\t\t#clean_sent = re.sub(r\"[\\\\s\\\\n\\\\r]+\", \" \", clean_sent)\n",
    "\t\twords = clean_sent.split(\" \")\n",
    "\t\twords = [w for w in words if w !=\"\"]\n",
    "\t\tstart = 0\n",
    "\t\tfor word in words:\n",
    "\t\t\toffset = sent[1].find(word, start)\n",
    "\t\t\tstart = offset+1\n",
    "\t\t\tif word not in unique_tokens:\n",
    "\t\t\t\tunique_tokens[word] = [(sent[0], offset)]\n",
    "\t\t\telse: \n",
    "\t\t\t\tunique_tokens[word].append((sent[0], offset))\n",
    "\treturn unique_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flush-concrete",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'उस': [(62001001, 0), (62001002, 73)],\n",
       " 'जीवन': [(62001001, 3), (62001002, 4), (62001002, 82)],\n",
       " 'के': [(62001001, 8),\n",
       "  (62001001, 15),\n",
       "  (62001002, 114),\n",
       "  (62001003, 124),\n",
       "  (62001003, 156)],\n",
       " 'वचन': [(62001001, 11), (62001010, 67)],\n",
       " 'विषय': [(62001001, 18)],\n",
       " 'में': [(62001001, 23),\n",
       "  (62001006, 58),\n",
       "  (62001007, 22),\n",
       "  (62001007, 51),\n",
       "  (62001008, 19),\n",
       "  (62001008, 74),\n",
       "  (62001009, 94),\n",
       "  (62001010, 74)],\n",
       " 'जो': [(62001001, 27), (62001002, 106), (62001003, 0), (62001005, 0)],\n",
       " 'आदि': [(62001001, 30)],\n",
       " 'से': [(62001001, 34),\n",
       "  (62001001, 77),\n",
       "  (62001001, 107),\n",
       "  (62001001, 124),\n",
       "  (62001007, 73),\n",
       "  (62001007, 139),\n",
       "  (62001009, 80)],\n",
       " 'था': [(62001001, 37), (62001002, 121)],\n",
       " 'जिसे': [(62001001, 42), (62001001, 61), (62001001, 91)],\n",
       " 'हमने': [(62001001, 47),\n",
       "  (62001001, 96),\n",
       "  (62001002, 23),\n",
       "  (62001003, 7),\n",
       "  (62001005, 10),\n",
       "  (62001010, 15)],\n",
       " 'सुना': [(62001001, 52), (62001003, 20), (62001005, 20)],\n",
       " 'और': [(62001001, 58),\n",
       "  (62001001, 115),\n",
       "  (62001002, 20),\n",
       "  (62001002, 38),\n",
       "  (62001002, 62),\n",
       "  (62001002, 124),\n",
       "  (62001003, 17),\n",
       "  (62001003, 98),\n",
       "  (62001003, 132),\n",
       "  (62001004, 0),\n",
       "  (62001005, 26),\n",
       "  (62001005, 83),\n",
       "  (62001006, 44),\n",
       "  (62001006, 87),\n",
       "  (62001007, 94),\n",
       "  (62001008, 68),\n",
       "  (62001009, 63),\n",
       "  (62001009, 111),\n",
       "  (62001010, 59)],\n",
       " 'अपनी': [(62001001, 66)],\n",
       " 'आँखों': [(62001001, 71)],\n",
       " 'देखा': [(62001001, 80), (62001001, 110), (62001002, 32), (62001003, 12)],\n",
       " 'वरन्': [(62001001, 86)],\n",
       " 'ध्यान': [(62001001, 101)],\n",
       " 'हाथों': [(62001001, 118)],\n",
       " 'छुआ': [(62001001, 127)],\n",
       " 'यह': [(62001002, 1), (62001003, 107), (62001005, 52)],\n",
       " 'प्रगट': [(62001002, 9), (62001002, 133)],\n",
       " 'हुआ': [(62001002, 15), (62001002, 139)],\n",
       " 'उसे': [(62001002, 28), (62001010, 38)],\n",
       " 'उसकी': [(62001002, 41)],\n",
       " 'गवाही': [(62001002, 46)],\n",
       " 'देते': [(62001002, 52), (62001002, 97), (62001003, 51), (62001008, 59)],\n",
       " 'हैं': [(62001002, 57),\n",
       "  (62001002, 102),\n",
       "  (62001003, 56),\n",
       "  (62001004, 27),\n",
       "  (62001005, 44),\n",
       "  (62001005, 79),\n",
       "  (62001007, 90),\n",
       "  (62001008, 64),\n",
       "  (62001010, 54)],\n",
       " 'तुम्हें': [(62001002, 65), (62001003, 40), (62001005, 29)],\n",
       " 'अनन्त': [(62001002, 76)],\n",
       " 'का': [(62001002, 87), (62001007, 118)],\n",
       " 'समाचार': [(62001002, 90), (62001003, 33), (62001005, 3)],\n",
       " 'पिता': [(62001002, 109), (62001003, 119)],\n",
       " 'साथ': [(62001002, 117),\n",
       "  (62001003, 83),\n",
       "  (62001003, 127),\n",
       "  (62001003, 159),\n",
       "  (62001006, 21)],\n",
       " 'हम': [(62001002, 127),\n",
       "  (62001004, 12),\n",
       "  (62001006, 4),\n",
       "  (62001006, 71),\n",
       "  (62001007, 38),\n",
       "  (62001008, 4),\n",
       "  (62001008, 16),\n",
       "  (62001008, 71),\n",
       "  (62001009, 4),\n",
       "  (62001010, 4),\n",
       "  (62001010, 71)],\n",
       " 'पर': [(62001002, 130), (62001006, 95), (62001007, 0)],\n",
       " 'कुछ': [(62001003, 3), (62001005, 92), (62001008, 23)],\n",
       " 'है': [(62001003, 25),\n",
       "  (62001003, 163),\n",
       "  (62001005, 55),\n",
       "  (62001006, 40),\n",
       "  (62001006, 84),\n",
       "  (62001007, 26),\n",
       "  (62001007, 153),\n",
       "  (62001009, 120),\n",
       "  (62001010, 83)],\n",
       " 'उसका': [(62001003, 28), (62001010, 62)],\n",
       " 'भी': [(62001003, 48),\n",
       "  (62001003, 74),\n",
       "  (62001005, 96),\n",
       "  (62001007, 41),\n",
       "  (62001008, 27)],\n",
       " 'इसलिए': [(62001003, 61), (62001004, 15)],\n",
       " 'कि': [(62001003, 67),\n",
       "  (62001004, 32),\n",
       "  (62001005, 59),\n",
       "  (62001006, 13),\n",
       "  (62001008, 13),\n",
       "  (62001010, 12)],\n",
       " 'तुम': [(62001003, 70)],\n",
       " 'हमारे': [(62001003, 77), (62001009, 36)],\n",
       " 'सहभागी': [(62001003, 87)],\n",
       " 'हो': [(62001003, 94), (62001004, 55)],\n",
       " 'हमारी': [(62001003, 101), (62001006, 25)],\n",
       " 'सहभागिता': [(62001003, 110), (62001006, 31), (62001007, 76)],\n",
       " 'उसके': [(62001003, 135), (62001006, 16), (62001007, 97)],\n",
       " 'पुत्र': [(62001003, 140), (62001007, 102)],\n",
       " 'यीशु': [(62001003, 146), (62001007, 108)],\n",
       " 'मसीह': [(62001003, 151), (62001007, 113)],\n",
       " 'ये': [(62001004, 3)],\n",
       " 'बातें': [(62001004, 6)],\n",
       " 'लिखते': [(62001004, 21)],\n",
       " 'तुम्हारा': [(62001004, 35)],\n",
       " 'आनन्द': [(62001004, 44)],\n",
       " 'पूरा': [(62001004, 50)],\n",
       " 'जाए': [(62001004, 58)],\n",
       " 'उससे': [(62001005, 15)],\n",
       " 'सुनाते': [(62001005, 37)],\n",
       " 'वह': [(62001005, 49), (62001007, 12), (62001009, 33)],\n",
       " 'परमेश्\\u200dवर': [(62001005, 62)],\n",
       " 'ज्योति': [(62001005, 72), (62001007, 15), (62001007, 44)],\n",
       " 'उसमें': [(62001005, 86)],\n",
       " 'अंधकार': [(62001005, 99), (62001006, 51)],\n",
       " 'नहीं': [(62001005, 106),\n",
       "  (62001006, 98),\n",
       "  (62001008, 34),\n",
       "  (62001008, 83),\n",
       "  (62001010, 24),\n",
       "  (62001010, 78)],\n",
       " 'यदि': [(62001006, 0),\n",
       "  (62001007, 3),\n",
       "  (62001008, 0),\n",
       "  (62001009, 0),\n",
       "  (62001010, 0)],\n",
       " 'कहें': [(62001006, 7), (62001008, 7), (62001010, 7)],\n",
       " 'फिर': [(62001006, 47)],\n",
       " 'चलें': [(62001006, 62), (62001007, 55)],\n",
       " 'तो': [(62001006, 68),\n",
       "  (62001007, 61),\n",
       "  (62001008, 40),\n",
       "  (62001009, 30),\n",
       "  (62001010, 35)],\n",
       " 'झूठ': [(62001006, 74)],\n",
       " 'बोलते': [(62001006, 78)],\n",
       " 'सत्य': [(62001006, 90), (62001008, 78)],\n",
       " 'चलते': [(62001006, 103)],\n",
       " 'जैसा': [(62001007, 7)],\n",
       " 'वैसे': [(62001007, 30)],\n",
       " 'ही': [(62001007, 35)],\n",
       " 'एक': [(62001007, 64)],\n",
       " 'दूसरे': [(62001007, 67)],\n",
       " 'रखते': [(62001007, 85)],\n",
       " 'लहू': [(62001007, 121)],\n",
       " 'हमें': [(62001007, 125), (62001009, 66)],\n",
       " 'सब': [(62001007, 130), (62001009, 71)],\n",
       " 'पापों': [(62001007, 133), (62001009, 12), (62001009, 42)],\n",
       " 'शुद्ध': [(62001007, 142), (62001009, 83)],\n",
       " 'करता': [(62001007, 148)],\n",
       " 'यशा': [(62001007, 158)],\n",
       " 'पाप': [(62001008, 30), (62001010, 20)],\n",
       " 'अपने': [(62001008, 43), (62001009, 7)],\n",
       " 'आप': [(62001008, 48)],\n",
       " 'को': [(62001008, 51), (62001009, 18), (62001009, 48)],\n",
       " 'धोखा': [(62001008, 54)],\n",
       " 'मान': [(62001009, 21)],\n",
       " 'लें': [(62001009, 25)],\n",
       " 'क्षमा': [(62001009, 51)],\n",
       " 'करने': [(62001009, 57), (62001009, 89)],\n",
       " 'अधर्म': [(62001009, 74)],\n",
       " 'विश्वासयोग्य': [(62001009, 98)],\n",
       " 'धर्मी': [(62001009, 114)],\n",
       " 'भज': [(62001009, 125)],\n",
       " 'नीति': [(62001009, 135)],\n",
       " 'किया': [(62001010, 29)],\n",
       " 'झूठा': [(62001010, 42)],\n",
       " 'ठहराते': [(62001010, 47)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences = [(62001001,\"उस जीवन के वचन के विषय में जो आदि से था*, जिसे हमने सुना, और जिसे अपनी आँखों से देखा, वरन् जिसे हमने ध्यान से देखा और हाथों से छुआ।\"),\n",
    "(62001002,\"(यह जीवन प्रगट हुआ, और हमने उसे देखा, और उसकी गवाही देते हैं, और तुम्हें उस अनन्त जीवन का समाचार देते हैं जो पिता के साथ था और हम पर प्रगट हुआ)।\"),\n",
    "(62001003,\"जो कुछ हमने देखा और सुना है उसका समाचार तुम्हें भी देते हैं, इसलिए कि तुम भी हमारे साथ सहभागी हो; और हमारी यह सहभागिता पिता के साथ, और उसके पुत्र यीशु मसीह के साथ है।\"),\n",
    "(62001004,\"और ये बातें हम इसलिए लिखते हैं, कि तुम्हारा आनन्द पूरा हो जाए*।\"),\n",
    "(62001005,\"जो समाचार हमने उससे सुना, और तुम्हें सुनाते हैं, वह यह है; कि परमेश्‍वर ज्योति हैं और उसमें कुछ भी अंधकार नहीं*।\"),\n",
    "(62001006,\"यदि हम कहें, कि उसके साथ हमारी सहभागिता है, और फिर अंधकार में चलें, तो हम झूठ बोलते है और सत्य पर नहीं चलते।\"),\n",
    "(62001007,\"पर यदि जैसा वह ज्योति में है, वैसे ही हम भी ज्योति में चलें, तो एक दूसरे से सहभागिता रखते हैं और उसके पुत्र यीशु मसीह का लहू हमें सब पापों से शुद्ध करता है। (यशा. 2:5)\"),\n",
    "(62001008,\"यदि हम कहें, कि हम में कुछ भी पाप नहीं, तो अपने आप को धोखा देते हैं और हम में सत्य नहीं।\"),\n",
    "(62001009,\"यदि हम अपने पापों को मान लें, तो वह हमारे पापों को क्षमा करने, और हमें सब अधर्म से शुद्ध करने में विश्वासयोग्य और धर्मी है। (भज. 32:5, नीति. 28:13)\"),\n",
    "(62001010,\"यदि हम कहें कि हमने पाप नहीं किया, तो उसे झूठा ठहराते हैं, और उसका वचन हम में नहीं है।\")]\n",
    "\n",
    "tokenize(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-serbia",
   "metadata": {},
   "source": [
    "## Get Text functions\n",
    "\n",
    "How about we define a get text function on every content table(like bible, commentary etc), which would return the cleaned text field contents along with an id for that specific table?\n",
    "\n",
    "*Answer*: Yes. But design it as an abstract class which is inherited and implemented for each kind of sources\n",
    "\n",
    "* This list of sentences could be used as input for tokenization and draft generation. \n",
    "* Also this could be used for apps like Autographa or BridgeEngine to display reference texts on screen, as it would contain just the clean contents and no foot notes, cross-refs, strongs markups, alignments or any other non-relevant contents in USFM files\n",
    "* This would also come in handy for model building scripts to get the texts from varoius content tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "structural-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "import schemas, db_models\n",
    "\n",
    "def get_text_from_bible(db_, source_name, ref_start:schemas.Reference=None, \n",
    "    ref_end:schemas.Reference=None):\n",
    "    '''fetched text contents from bible_cleaned tables to be used for translations apps \n",
    "    or for model building.\n",
    "    Output format: [(id, sentance), (id, sentance), ....]'''\n",
    "    if source_name not in db_models.dynamicTables:\n",
    "        raise NotAvailableException('%s not found in database.'%source_name)\n",
    "    if not source_name.endswith('_bible'):\n",
    "        raise TypeException('The operation is supported only on bible')\n",
    "    model_cls = db_models.dynamicTables[source_name+'_cleaned']\n",
    "    ref_id_start = ref_id_end = None\n",
    "    if ref_start:\n",
    "    \tbook = db_models.BibleBook.filter(db_models.BibleBook.bookCode == ref_start.bookCode).first()\n",
    "    \tif not book:\n",
    "    \t\traise NotAvailableException(\"Book %s, not found in database\"%ref_start.bookCode)\n",
    "    \tref_id_start = book.bookId*1000000 + ref_start.chapter*1000 + ref_start.verseNumber\n",
    "    if ref_end:\n",
    "    \tbook = db_models.BibleBook.filter(db_models.BibleBook.bookCode == ref_end.bookCode).first()\n",
    "    \tif not book:\n",
    "    \t\traise NotAvailableException(\"Book %s, not found in database\"%ref_end.bookCode)\n",
    "    \tref_id_end = book.bookId*1000000 + ref_end.chapter*1000 + ref_end.verseNumber\n",
    "    if not ref_id_start:\n",
    "    \tref_id_start = 0\n",
    "    if not ref_id_end:\n",
    "    \tref_id_end = 999999999\n",
    "    query = db_.query(model_cls).filter(model_cls.refId >= ref_id_start,\n",
    "    \tmodel_cls.refId <= ref_id_end, model_cls.active == True)\n",
    "    res = query.all()\n",
    "    formatted_res = []\n",
    "    for item in res:\n",
    "        formatted_res.append((item.refId, item.verseText))\n",
    "    return formatted_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "environmental-plymouth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(41001001, 'इब्राहीम की सन्\\u200dतान, दाऊद की ...')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from database import SessionLocal, engine\n",
    "db_ = SessionLocal()\n",
    "\n",
    "get_text_from_bible(db_, source_name=\"hin_IRV_1_bible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-booking",
   "metadata": {},
   "source": [
    "## Draft Generation\n",
    "\n",
    "In V1, draft generation was done by find and replace of tokens(in the descending order of length of token) on the USFM file.\n",
    "\n",
    "In V2, as we are doing context based translation, it needs to be changed. We will be doing a replacement of tokens with translations on specific occurance.\n",
    "\n",
    "How about we do not use the input(source/reference USFM) for this replacement, instead create a fresh minimal USFM with the translated verses? For this we will be using the clean verse text we extracted from USFM and obtained using the the `get_text_from_bible()` function(the same text given for tokenizarion and displaying on UI), translate it using token replacement and then attach the minimum required markers \\id, \\c, \\p and \\v appropriately.\n",
    "\n",
    "*Answer*: Yes\n",
    "\n",
    "By doing this, all non-verse contents present in the source/reference USFM would be absent in the generated draft. \n",
    "\n",
    "An existing issue in the draft of V1 is that some words are not replaced with translations even though, they are present in tokens list and translated there. I think the issue happens because they are part of phrase tokens and these phrases are broken apart in USFM file with additional markup in between them. So the find and replace doesnt work. Similar issues will occur for us in V2 also even if we are using offsets. So I think, using the cleaned text for replacement would be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(db_, project_id, sent_ids=None):\n",
    "    '''does token replacement translation with translation memory'''\n",
    "    #fetch sentences from source document\n",
    "    if sent_ids is None:\n",
    "        sentences = db_.query(db_models.TranslationMemory.sourceDocument).filter(\n",
    "            db_models.TranslationMemory.projectId == project_id).first()\n",
    "    else:\n",
    "        sentences = db_.query(db_models.TranslationMemory.sourceDocument).filter(\n",
    "            db_models.TranslationMemory.projectId == project_id,\n",
    "            db_models.TranslationMemory.sentences).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-packet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-logistics",
   "metadata": {},
   "source": [
    "## Suggestions\n",
    "\n",
    "For every language pair for which we have translation projects or previously available parallel aligned data, we will have a *translation memory learned model*. When we encounter a word in a particular context, we use this learned model to get all possible translations of the word and get them scored based on the current context window. These scored translation can be given to user as suggestions.\n",
    "\n",
    "I think the key thing to decide in this is, how do we store the translation memory to be used efficiently.\n",
    "Or what do we mean by a learned translation model for a language pair?\n",
    "\n",
    "The options that occur to me are as follows:\n",
    "1. Query the translation memory(alignment) **SQL table** based on the key word and check and sort them based on the context window(I am afraid, this will have poor performace in terms of time and space)\n",
    "2. Periodically build a **trie** structure from translation memory(alignment) table and query this trie for suggestions. I am not yet familiar with trie. I hope it allows us to search based on a context window efficiently. One draw back I can see in this is \"learning\" will not happen in real time and data user adds will take time(depending on how often we run the learning script) to improve the suggestions quality.\n",
    "3. While we keep a translation memory table in SQL DB, parallely bulid a **graph** structure with it in DGraph. Use this graph for suggestions and use the table in SQL DB for draft generation. \n",
    "4. Build a **nueral network** (or ML) model that can be trained with word and context window and can predict the translation. I am not sure if we can get such a model to give multiple translations with varing scores. Building, storing, and using such models can also become expensive in terms of time and space. \n",
    "\n",
    "*Answer*: Option 2, trie built from SQL table, for now and 3, 4 for later\n",
    "\n",
    "**Proposed tire structure for AgMT**\n",
    "\n",
    "* Have one trie per source-target language pair, this would reduce the size and thus increase search performance at level 1\n",
    "* Each node will have\n",
    "\t* a key: the context. The window size increases by one at each level\n",
    "\t* translations: list of all seen translations and their count for the given context. The count and current level can be used for scoring suggestions(score = level*count/total_occurances, total_occurance=sumOfCountsAtLevel1)\n",
    "\t* children: context increases by one word to right or left from the current context\n",
    "\n",
    "input: \n",
    "```\n",
    "[\n",
    "{\"token\": \"house\",\"context\":\"They use barrels to house their pets\",\"translation\":\"പാര്‍പ്പിക്കുക\"},\n",
    "{\"token\": \"house\",\"context\":\"His house is to the left\",\"translation\":\"വീട്\"},\n",
    "{\"token\": \"house\",\"context\":\"Their house contruction methods are different\",\"translation\":\"ഭവന\"},\n",
    "{\"token\": \"house\",\"context\":\"Last time I went to his house,\",\"translation\":\"വീട്ടിലേക്ക്\"},\n",
    "{\"token\": \"house\",\"context\":\"Museums house large collection of Roman sculpture\",\"translation\":\"ഉള്‍ക്കൊള്ളുന്നു\"}\n",
    "]\n",
    "```\n",
    "A trie of window size 3\n",
    "![trie diagram](example_trie.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "relevant-batch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house', 'house/L:his', 'house/R:is', 'house/L:his/R:is', 'house/R:is/L:his']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def form_trie_keys(prefix, to_left, to_right, prev_keys):\n",
    "    '''build the trie tree recursively'''    \n",
    "    keys = prev_keys\n",
    "    a = b = None\n",
    "    if len(to_left) > 0:\n",
    "        a = '/L:'+to_left.pop(0)\n",
    "    if len(to_right) > 0:\n",
    "        b = '/R:'+to_right.pop(0)\n",
    "    if a:\n",
    "        key_left = prefix + a\n",
    "        keys.append(key_left)\n",
    "        if not b:\n",
    "            keys = form_trie_keys(key_left, to_left.copy(), to_right.copy(), keys)\n",
    "    if b:\n",
    "        key_right = prefix + b\n",
    "        keys.append(key_right)\n",
    "        if not a:\n",
    "            keys = form_trie_keys(key_right, to_left.copy(), to_right.copy(), keys)\n",
    "    if a and b:\n",
    "        key_both_1 = prefix + a + b\n",
    "        key_both_2 = prefix + b + a\n",
    "        keys.append(key_both_1)\n",
    "        keys.append(key_both_2)\n",
    "        keys = form_trie_keys(key_both_1, to_left.copy(), to_right.copy(), keys)\n",
    "        keys = form_trie_keys(key_both_2, to_left, to_right, keys)\n",
    "    return keys\n",
    "\n",
    "token = \"house\"\n",
    "#context = [\"house\"]\n",
    "context = [\"his\", \"house\", \"is\"]\n",
    "#context = [\"his\", \"house\"]\n",
    "#context = [\"house\", \"is\"]\n",
    "#context = [\"says\",\"his\", \"house\", \"is\", \"in\", \"town\"]\n",
    "#context = [\"house\", \"is\", \"in\", \"town\"]\n",
    "#context = [\"he\",\"says\",\"his\", \"house\"]\n",
    "#context = [\"he\",\"says\",\"his\", \"house\", \"is\", \"in\", \"town\"]\n",
    "\n",
    "token_index = context.index(token)\n",
    "to_left = [context[i] for i in range(token_index-1, -1, -1)]\n",
    "to_right = context[token_index+1:]\n",
    "form_trie_keys(token, to_left, to_right, [token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "local-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygtrie\n",
    "\n",
    "def build_trie(token_context__trans_list):\n",
    "    '''Build a trie tree from scratch\n",
    "    input: [(token,context_list, translation), ...]'''\n",
    "    t = pygtrie.StringTrie()\n",
    "    for item in token_context__trans_list:\n",
    "        token = item[0]\n",
    "        context = item[1]\n",
    "        translation = item[2]\n",
    "        token_index = context.index(token)\n",
    "        to_left = [context[i] for i in range(token_index-1, -1, -1)]\n",
    "        to_right = context[token_index+1:]\n",
    "        keys = form_trie_keys(token, to_left, to_right, [token])\n",
    "        for key in keys:\n",
    "            if t.has_key(key):\n",
    "                value = t[key]\n",
    "                if translation in value.keys():\n",
    "                    value[translation] += 1\n",
    "                else:\n",
    "                    value[translation] = 1\n",
    "                t[key] = value\n",
    "            else:\n",
    "                t[key] = {translation: 1}\n",
    "    return t\n",
    "\n",
    "training_data = [\n",
    "    (\"bank\", [\"bank\", \"is\", \"closed\"], \"ബാങ്ക്\"),\n",
    "    (\"bank\", [\"they\", \"bank\", \"on\", \"us\"], \"ആശ്രയിക്കുക\"),\n",
    "    (\"bank\", [\"pay\", \"bank\", \"back\"], \"ബാങ്ക്\"),\n",
    "    (\"bank\", [\"river\", \"bank\", \"is\", \"muddy\"], \"തീരം\"),\n",
    "    (\"bank\", [\"bank\", \"manager\", \"spoke\"], \"ബാങ്ക്\")]\n",
    "t = build_trie(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "metropolitan-liverpool",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ബാങ്ക്', 0.6), ('ആശ്രയിക്കുക', 0.2), ('തീരം', 0.2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_translation_suggestion(word, context, t):\n",
    "    '''find the context based translation suggestions for a word.\n",
    "    Makes use of the learned model, t, for the lang pair, based on translation memory\n",
    "    output format: [(translation1, score1), (translation2, score2), ...]'''\n",
    "    token_index = context.index(word)\n",
    "    to_left = [context[i] for i in range(token_index-1, -1, -1)]\n",
    "    to_right = context[token_index+1:]\n",
    "    keys = form_trie_keys(word, to_left, to_right, [word])\n",
    "    suggestions = {}\n",
    "    single_word_match = t[word]\n",
    "    total_count = sum(single_word_match.values())\n",
    "    for k in keys:\n",
    "        match = t.longest_prefix(k)\n",
    "        levels = len(match.key.split(\"/\"))\n",
    "        for trans in match.value:\n",
    "            score = match.value[trans]*levels*levels / total_count\n",
    "            if trans in suggestions:\n",
    "                if suggestions[trans] < score:\n",
    "                    suggestions[trans] = score\n",
    "            else:\n",
    "                suggestions[trans] = score\n",
    "    return [(key, suggestions[key]) for key in suggestions]\n",
    "\n",
    "#get_translation_suggestion(\"bank\", [\"pay\", \"bank\", \"the\", \"money\"], t)\n",
    "#get_translation_suggestion(\"bank\", [\"bank\", \"the\", \"money\"], t)\n",
    "#get_translation_suggestion(\"bank\", [\"river\", \"bank\", \"is\", \"near\"], t)\n",
    "#get_translation_suggestion(\"bank\", [\"people\", \"bank\", \"on\", \"others\"], t)\n",
    "get_translation_suggestion(\"bank\", [\"bank\"], t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-competition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-sellers",
   "metadata": {},
   "source": [
    "## Tables\n",
    "\n",
    "![Projects and Translation memory tables](agmt_tables.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-perry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-trance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
