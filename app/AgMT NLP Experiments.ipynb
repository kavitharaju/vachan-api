{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "resident-attitude",
   "metadata": {},
   "source": [
    "### Some Previous Discussions\n",
    "\n",
    "[Points noted down to be improved in previous AgMT APIs](https://teams.microsoft.com/l/file/A8BD6EC4-4946-482C-A7C0-5DFA0AC96ACA?tenantId=dc5352cb-2fb1-4f19-a355-61b4398ec2e1&fileType=docx&objectUrl=https%3A%2F%2Fbridgeconn.sharepoint.com%2Fsites%2FDevTeam-AgMT-VachanAPI%2FShared%20Documents%2FAgMT%20-%20VachanAPI%2FAPI%20Refactoring%2FAgMT%20API%20Revision.docx&baseUrl=https%3A%2F%2Fbridgeconn.sharepoint.com%2Fsites%2FDevTeam-AgMT-VachanAPI&serviceName=teams&threadId=19:eafa29b748664314b67c8a8105d7caec@thread.tacv2&groupId=0d8df138-370a-4ec7-917d-8ec0699577f6)\n",
    "\n",
    "[The discussion document on suggestions module](https://teams.microsoft.com/l/file/44E2A83F-30FC-49CA-8E74-5BDDF1824DFC?tenantId=dc5352cb-2fb1-4f19-a355-61b4398ec2e1&fileType=docx&objectUrl=https%3A%2F%2Fbridgeconn.sharepoint.com%2Fsites%2FDevTeam-AgMT-VachanAPI%2FShared%20Documents%2FAgMT%20-%20VachanAPI%2FAPI%20Refactoring%2FSuggestions%20Module.docx&baseUrl=https%3A%2F%2Fbridgeconn.sharepoint.com%2Fsites%2FDevTeam-AgMT-VachanAPI&serviceName=teams&threadId=19:eafa29b748664314b67c8a8105d7caec@thread.tacv2&groupId=0d8df138-370a-4ec7-917d-8ec0699577f6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-facility",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-warehouse",
   "metadata": {},
   "source": [
    "How about we use single word tokens for now(beta release in June)?\n",
    "\n",
    "Issues in using phrases\n",
    "1. The best way to capture alignments is to use single word tokens. As we plan to use alignments to automaticaly identify token translation and enrich translation memory, using single word tokens would be easier to begin with. As we are planning to give context based suggestions, phrases of at least 3 words would always be considered in effect. So we may still get the quality improvement of using phrase tokens instead of single word.\n",
    "2. Using phrase tokens increase the number of tokens to be translated considerably. Translators doesn't seem very happy about it. Now that context based translations are also going to be encouraged, they may feel their work is too much in token translation phase.\n",
    "\n",
    "**Answer**: No, better use phrases now itself or the design choices we make now would make it not possible to upgrade later\n",
    "\n",
    "Main changes from older method(V1)\n",
    "* The fucntion below takes **any list of sentences** as input(as list of (id, sentence) tuples) and tokenize them into single word tokens. This gives us flexibility to do tokenization in any desired manner: whole bible at once, some books, some chapters of a book etc. Later, in Autographa or outside, the same function can be used to translate other text contents like commenatries, notes or stories.\n",
    "* We **dont use statistical models** for phrase identification. That makes it possible to work with data of very less size too (like one story)\n",
    "* we get more control over tokenization. For example we can load a set of predefined phrases to the translation memory table and make sure they get treated as tokens(eg: translation words, named entities etc)\n",
    "* we can **use language specific knowledge** like stopwords, puctuations etc if available\n",
    "* tokenization **changes(improves) with user data**. As translation memory table is added with newer phrases for a language, from different projects in the App or alignment data obtained from the app or otherwise, the tokens may change accordingly.\n",
    "* The tokens are returned in chronological order, that is **in the order they appear in the input text**. I hope this will give the user a better connection to the source while translating and also a better idea about the progress he is making through the source text.\n",
    "* The tokens are returned as a dict/json and the value of each token-key would be **the list of occurances** of the token. This would become handy for UI app for highlighlighting occurances and also returning the occurances where each sense is to be applied back to server. \n",
    "* In the **absence of any prior knowledge**, ie. no phrases in translation memory and no known stopwords for that source language, the base line system would give single word tokens only\n",
    "\n",
    "In the UI, the user should be allowed to **fiddle with the machine generated tokens**. He should be able to split a phrase token into component words, if he finds that suits translation better. Also he should be allowed to combine adjacent tokens to form bigger phrases. This would allow us to learn better phrases and would be easier for user than going to a separate alignment mode for it.(I am not suggesting replacing alignment mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alive-exposure",
   "metadata": {
    "code_folding": [
     11
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking input: hello! My dear friend. It's a tasty, healthy food. Did you make it? Thanks!!! \n",
      "Chunking output: ['hello', 'My dear friend', 'It', 's a tasty', 'healthy food', 'Did you make it', 'Thanks', '']\n",
      "\n",
      "\n",
      "phrases input(one chunk): उस जीवन के वचन के विषय में जो आदि से था\n",
      "phrases output:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['उस जीवन के', 'वचन के', 'विषय में जो', 'आदि से था']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chunking based on punctuations \n",
    "import re\n",
    "import utils\n",
    "import pdb\n",
    "punctuations = utils.punctuations()+utils.numbers()\n",
    "sample_text = \"hello! My dear friend. It's a tasty, healthy food. Did you make it? Thanks!!! \"\n",
    "chunks = [chunk.strip() for chunk in re.split(r'['+\"\".join(punctuations)+']+', sample_text)]\n",
    "print('Chunking input:', sample_text)\n",
    "print('Chunking output:',chunks)\n",
    "\n",
    "# matching phrase alternate\n",
    "def find_phrases(text, stop_words):\n",
    "    '''try forming phrases as <preposition stop word>* <content word> <postposition stop word>*'''\n",
    "    #pdb.set_trace()\n",
    "    words = text.split()\n",
    "    phrases = []\n",
    "    current_phrase = ''\n",
    "    state = 'pre'\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        word = words[i]\n",
    "        if state == 'pre':\n",
    "            if word in stop_words['prepositions']:\n",
    "                current_phrase += ' '+ word # adds prepostion, staying in 'pre' state\n",
    "            else:\n",
    "                current_phrase += ' '+ word # adds one content word and goes to 'post' state\n",
    "                state = 'post'\n",
    "        elif state == 'post':\n",
    "            if word in stop_words['postpositions']:\n",
    "                current_phrase += ' '+ word # adds postposition, staying in 'post' state\n",
    "            else:\n",
    "                phrases.append(current_phrase.strip()) # stops the phrase building\n",
    "                current_phrase = word\n",
    "                if word in stop_words['prepositions']:\n",
    "                    state = 'pre'\n",
    "                else:\n",
    "                    state = 'post'\n",
    "        #pdb.set_trace()\n",
    "        i += 1\n",
    "        \n",
    "    phrases.append(current_phrase.strip())\n",
    "    return phrases\n",
    "\n",
    "text = 'उस जीवन के वचन के विषय में जो आदि से था'\n",
    "stop_words = utils.stopwords('hin')\n",
    "\n",
    "print('\\n\\nphrases input(one chunk):', text)\n",
    "print('phrases output:')\n",
    "find_phrases(text, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "indonesian-parking",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def display_tree(tree):\n",
    "    for path in tree.items():\n",
    "        nodes = path[0].split('/')\n",
    "        for nod in nodes:\n",
    "            print('\\t-',nod,end='')\n",
    "        print(' => ',path[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "hidden-greenhouse",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- जीवन\t- के\t- वचन =>  0\n",
      "\t- जीवन\t- का =>  0\n",
      "\t- अपनी\t- आँखों\t- से\t- देखा =>  0\n",
      "\t- पिता\t- के\t- साथ =>  0\n",
      "\t- यीशु\t- मसीह =>  0\n",
      "\t- परमेश्‍वर\t- ज्योति =>  0\n",
      "\t- झूठा\t- ठहराते =>  0\n",
      "\t- Here\t- is\t- it =>  0\n",
      "\t- hare =>  0\n",
      "\t- no =>  0\n"
     ]
    }
   ],
   "source": [
    "import pygtrie, re\n",
    "\n",
    "def build_memory_trie(translation_memory):\n",
    "    memory_trie = pygtrie.StringTrie()\n",
    "    space_pattern = re.compile('\\s+')\n",
    "    for token in translation_memory:\n",
    "        key = re.sub(space_pattern,'/', token)\n",
    "        memory_trie[key] = 0\n",
    "    return memory_trie\n",
    "\n",
    "#fetch all distinct tokens for the source language from translation memory in DB\n",
    "mock_translation_memory = [\"जीवन के वचन\", \"जीवन का\", \"अपनी आँखों से देखा\", \"पिता के साथ\", \"यीशु मसीह\", \"परमेश्‍वर ज्योति\", \"झूठा ठहराते\",\n",
    "                          \"Here is it\", \"hare\", \"no\"]\n",
    "\n",
    "#build a trie using the fetched data. words in tokens will form the path \n",
    "memory_trie = build_memory_trie(mock_translation_memory)\n",
    "display_tree(memory_trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "secure-problem",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import utils\n",
    "import pdb\n",
    "\n",
    "def tokenize(src_lang, sent_list, punctuations=None,\n",
    "            stop_words = None):\n",
    "    '''Get phrase and single word tokens and their occurances from input sentence list.\n",
    "    Performs tokenization using two knowledge sources: translation memory and stopwords list\n",
    "    input: [(sent_id, sent_text), (sent_id, sent_text), ...]\n",
    "    output: {\"token\": [(sent_id, start_offset, end_offset),(sent_id, start_offset, end_offset)..],\n",
    "             \"token\": [(sent_id, start_offset, end_offset),(sent_id, start_offset, end_offset)..], ...}'''\n",
    "    #pdb.set_trace()\n",
    "    unique_tokens = {}\n",
    "    if stop_words is None:\n",
    "        stop_words = utils.stopwords(src_lang)\n",
    "    if punctuations is None:\n",
    "        punctuations = utils.punctuations()+utils.numbers()\n",
    "    # fetch all known tokens for the language and build a trie with it\n",
    "    memory_trie = build_memory_trie(mock_translation_memory)\n",
    "    for sent in sent_list:\n",
    "        phrases = []\n",
    "        text = re.sub(r'[\\n\\r]+', ' ', sent[1])\n",
    "        #first split the text into chunks based on punctuations\n",
    "        chunks = [chunk.strip() for chunk in re.split(r'['+\"\".join(punctuations)+']+', text)]\n",
    "        updated_chunks = []\n",
    "        for i,chunk in enumerate(chunks):\n",
    "            #search the trie to get the longest matching phrases known to us\n",
    "            temp = chunk\n",
    "            new_chunks = ['']\n",
    "            while temp != \"\":\n",
    "                key = '/'.join(temp.split())\n",
    "                lngst = memory_trie.longest_prefix(key)\n",
    "                if lngst.key is not None:\n",
    "                    new_chunks.append(\"###\"+lngst.key.replace('/',' '))\n",
    "                    temp = temp[len(lngst.key):]\n",
    "                    new_chunks.append('')\n",
    "                else:\n",
    "                    if \" \" in temp:\n",
    "                        indx = temp.index(' ')\n",
    "                        new_chunks[-1] += temp[:indx+1]\n",
    "                        temp = temp[indx+1:]\n",
    "                    else:\n",
    "                        new_chunks[-1] += temp\n",
    "                        temp = \"\"\n",
    "                #pdb.set_trace()\n",
    "            updated_chunks += new_chunks\n",
    "            #pdb.set_trace()\n",
    "        chunks = [ chk.strip() for chk in updated_chunks if chk.strip() != '']       \n",
    "        for i,chunk in enumerate(chunks):\n",
    "            # from the left out words in above step, try forming phrases \n",
    "            # as <preposition stop word>* <content word> <postposition stop word>* \n",
    "            if chunk.startswith('###'):\n",
    "                phrases.append(chunk.replace(\"###\",\"\"))\n",
    "            else:\n",
    "                phrases+=find_phrases(chunk,stop_words)\n",
    "        start = 0\n",
    "        for phrase in phrases:\n",
    "            offset = sent[1].find(phrase, start)\n",
    "            if offset == -1:\n",
    "                #raise \"token not found in sentence\"\n",
    "                pdb.set_trace()\n",
    "            start = offset+1\n",
    "            if phrase not in unique_tokens:\n",
    "                unique_tokens[phrase] = [(sent[0], offset, offset+len(phrase))]\n",
    "            else: \n",
    "                unique_tokens[phrase].append((sent[0], offset, offset+len(phrase)))\n",
    "    return unique_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flush-concrete",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'उस': [(62001001, 0, 2)],\n",
       " 'जीवन के वचन': [(62001001, 3, 14)],\n",
       " 'के': [(62001001, 8, 10)],\n",
       " 'विषय में जो': [(62001001, 18, 29)],\n",
       " 'आदि से था': [(62001001, 30, 39)],\n",
       " 'जिसे': [(62001001, 42, 46), (62001001, 61, 65), (62001001, 91, 95)],\n",
       " 'हमने': [(62001001, 47, 51),\n",
       "  (62001001, 96, 100),\n",
       "  (62001002, 23, 27),\n",
       "  (62001005, 10, 14),\n",
       "  (62001010, 15, 19)],\n",
       " 'सुना': [(62001001, 52, 56), (62001005, 20, 24)],\n",
       " 'और': [(62001001, 58, 60),\n",
       "  (62001002, 20, 22),\n",
       "  (62001002, 38, 40),\n",
       "  (62001002, 62, 64),\n",
       "  (62001003, 98, 100),\n",
       "  (62001003, 132, 134),\n",
       "  (62001005, 26, 28),\n",
       "  (62001006, 44, 46),\n",
       "  (62001009, 63, 65),\n",
       "  (62001010, 59, 61)],\n",
       " 'अपनी आँखों से देखा': [(62001001, 66, 84)],\n",
       " 'वरन्': [(62001001, 86, 90)],\n",
       " 'ध्यान से': [(62001001, 101, 109)],\n",
       " 'देखा और': [(62001001, 110, 117), (62001003, 12, 19)],\n",
       " 'हाथों से': [(62001001, 118, 126)],\n",
       " 'छुआ': [(62001001, 127, 130)],\n",
       " 'यह जीवन': [(62001002, 1, 8)],\n",
       " 'प्रगट हुआ': [(62001002, 9, 18), (62001002, 133, 142)],\n",
       " 'उसे': [(62001002, 28, 31), (62001010, 38, 41)],\n",
       " 'देखा': [(62001002, 32, 36)],\n",
       " 'उसकी': [(62001002, 41, 45)],\n",
       " 'गवाही': [(62001002, 46, 51)],\n",
       " 'देते हैं': [(62001002, 52, 60), (62001003, 51, 59)],\n",
       " 'तुम्हें': [(62001002, 65, 72), (62001003, 40, 47), (62001005, 29, 36)],\n",
       " 'उस अनन्त': [(62001002, 73, 81)],\n",
       " 'जीवन का': [(62001002, 82, 89)],\n",
       " 'समाचार': [(62001002, 90, 96), (62001003, 33, 39), (62001005, 3, 9)],\n",
       " 'देते हैं जो': [(62001002, 97, 108)],\n",
       " 'पिता के साथ': [(62001002, 109, 120), (62001003, 119, 130)],\n",
       " 'था और': [(62001002, 121, 126)],\n",
       " 'हम पर': [(62001002, 127, 132)],\n",
       " 'जो': [(62001003, 0, 2), (62001005, 0, 2)],\n",
       " 'कुछ हमने': [(62001003, 3, 11)],\n",
       " 'सुना है': [(62001003, 20, 27)],\n",
       " 'उसका': [(62001003, 28, 32), (62001010, 62, 66)],\n",
       " 'भी': [(62001003, 48, 50), (62001003, 74, 76), (62001007, 41, 43)],\n",
       " 'इसलिए कि': [(62001003, 61, 69)],\n",
       " 'तुम': [(62001003, 70, 73)],\n",
       " 'हमारे साथ': [(62001003, 77, 86)],\n",
       " 'सहभागी हो': [(62001003, 87, 96)],\n",
       " 'हमारी': [(62001003, 101, 106), (62001006, 25, 30)],\n",
       " 'यह सहभागिता': [(62001003, 107, 118)],\n",
       " 'उसके': [(62001003, 135, 139), (62001007, 97, 101)],\n",
       " 'पुत्र': [(62001003, 140, 145), (62001007, 102, 107)],\n",
       " 'यीशु मसीह': [(62001003, 146, 155), (62001007, 108, 117)],\n",
       " 'के साथ है': [(62001003, 156, 165)],\n",
       " 'और ये': [(62001004, 0, 5)],\n",
       " 'बातें': [(62001004, 6, 11)],\n",
       " 'हम': [(62001004, 12, 14),\n",
       "  (62001006, 4, 6),\n",
       "  (62001006, 71, 73),\n",
       "  (62001007, 38, 40),\n",
       "  (62001008, 4, 6),\n",
       "  (62001009, 4, 6),\n",
       "  (62001010, 4, 6)],\n",
       " 'इसलिए': [(62001004, 15, 20)],\n",
       " 'लिखते हैं': [(62001004, 21, 30)],\n",
       " 'कि': [(62001004, 32, 34),\n",
       "  (62001005, 59, 61),\n",
       "  (62001006, 13, 15),\n",
       "  (62001008, 13, 15)],\n",
       " 'तुम्हारा': [(62001004, 35, 43)],\n",
       " 'आनन्द': [(62001004, 44, 49)],\n",
       " 'पूरा हो': [(62001004, 50, 57)],\n",
       " 'जाए': [(62001004, 58, 61)],\n",
       " 'उससे': [(62001005, 15, 19)],\n",
       " 'सुनाते हैं': [(62001005, 37, 47)],\n",
       " 'वह': [(62001005, 49, 51), (62001007, 12, 14), (62001009, 33, 35)],\n",
       " 'यह है': [(62001005, 52, 57)],\n",
       " 'परमेश्\\u200dवर ज्योति': [(62001005, 62, 78)],\n",
       " 'हैं और': [(62001005, 79, 85)],\n",
       " 'उसमें': [(62001005, 86, 91)],\n",
       " 'कुछ भी': [(62001005, 92, 98), (62001008, 23, 29)],\n",
       " 'अंधकार नहीं': [(62001005, 99, 110)],\n",
       " 'यदि': [(62001006, 0, 3),\n",
       "  (62001007, 3, 6),\n",
       "  (62001008, 0, 3),\n",
       "  (62001009, 0, 3),\n",
       "  (62001010, 0, 3)],\n",
       " 'कहें': [(62001006, 7, 11), (62001008, 7, 11)],\n",
       " 'उसके साथ': [(62001006, 16, 24)],\n",
       " 'सहभागिता है': [(62001006, 31, 42)],\n",
       " 'फिर': [(62001006, 47, 50)],\n",
       " 'अंधकार में': [(62001006, 51, 61)],\n",
       " 'चलें': [(62001006, 62, 66), (62001007, 55, 59)],\n",
       " 'तो': [(62001006, 68, 70),\n",
       "  (62001007, 61, 63),\n",
       "  (62001008, 40, 42),\n",
       "  (62001009, 30, 32),\n",
       "  (62001010, 35, 37)],\n",
       " 'झूठ': [(62001006, 74, 77)],\n",
       " 'बोलते है और': [(62001006, 78, 89)],\n",
       " 'सत्य पर नहीं': [(62001006, 90, 102)],\n",
       " 'चलते': [(62001006, 103, 107)],\n",
       " 'पर': [(62001007, 0, 2)],\n",
       " 'जैसा': [(62001007, 7, 11)],\n",
       " 'ज्योति में है': [(62001007, 15, 28)],\n",
       " 'वैसे ही': [(62001007, 30, 37)],\n",
       " 'ज्योति में': [(62001007, 44, 54)],\n",
       " 'एक': [(62001007, 64, 66)],\n",
       " 'दूसरे से': [(62001007, 67, 75)],\n",
       " 'सहभागिता': [(62001007, 76, 84)],\n",
       " 'रखते हैं और': [(62001007, 85, 96)],\n",
       " 'का': [(62001007, 118, 120)],\n",
       " 'लहू': [(62001007, 121, 124)],\n",
       " 'हमें': [(62001007, 125, 129), (62001009, 66, 70)],\n",
       " 'सब': [(62001007, 130, 132), (62001009, 71, 73)],\n",
       " 'पापों से': [(62001007, 133, 141)],\n",
       " 'शुद्ध करता है': [(62001007, 142, 155)],\n",
       " 'यशा': [(62001007, 158, 161)],\n",
       " 'हम में': [(62001008, 16, 22), (62001008, 71, 77)],\n",
       " 'पाप नहीं': [(62001008, 30, 38)],\n",
       " 'अपने': [(62001008, 43, 47), (62001009, 7, 11)],\n",
       " 'आप को': [(62001008, 48, 53)],\n",
       " 'धोखा': [(62001008, 54, 58)],\n",
       " 'देते हैं और': [(62001008, 59, 70)],\n",
       " 'सत्य नहीं': [(62001008, 78, 87)],\n",
       " 'पापों को': [(62001009, 12, 20), (62001009, 42, 50)],\n",
       " 'मान': [(62001009, 21, 24)],\n",
       " 'लें': [(62001009, 25, 28)],\n",
       " 'हमारे': [(62001009, 36, 41)],\n",
       " 'क्षमा करने': [(62001009, 51, 61)],\n",
       " 'अधर्म से': [(62001009, 74, 82)],\n",
       " 'शुद्ध करने में': [(62001009, 83, 97)],\n",
       " 'विश्वासयोग्य और': [(62001009, 98, 113)],\n",
       " 'धर्मी है': [(62001009, 114, 122)],\n",
       " 'भज': [(62001009, 125, 127)],\n",
       " 'नीति': [(62001009, 135, 139)],\n",
       " 'कहें कि': [(62001010, 7, 14)],\n",
       " 'पाप नहीं किया': [(62001010, 20, 33)],\n",
       " 'झूठा ठहराते': [(62001010, 42, 53)],\n",
       " 'हैं': [(62001010, 54, 57)],\n",
       " 'वचन': [(62001010, 67, 70)],\n",
       " 'हम में नहीं है': [(62001010, 71, 85)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences = [(62001001,\"उस जीवन के वचन के विषय में जो आदि से था*, जिसे हमने सुना, और जिसे अपनी आँखों से देखा, वरन् जिसे हमने ध्यान से देखा और हाथों से छुआ।\"),\n",
    "(62001002,\"(यह जीवन प्रगट हुआ, और हमने उसे देखा, और उसकी गवाही देते हैं, और तुम्हें उस अनन्त जीवन का समाचार देते हैं जो पिता के साथ था और हम पर प्रगट हुआ)।\"),\n",
    "(62001003,\"जो कुछ हमने देखा और सुना है उसका समाचार तुम्हें भी देते हैं, इसलिए कि तुम भी हमारे साथ सहभागी हो; और हमारी यह सहभागिता पिता के साथ, और उसके पुत्र यीशु मसीह के साथ है।\"),\n",
    "(62001004,\"और ये बातें हम इसलिए लिखते हैं, कि तुम्हारा आनन्द पूरा हो जाए*।\"),\n",
    "(62001005,\"जो समाचार हमने उससे सुना, और तुम्हें सुनाते हैं, वह यह है; कि परमेश्‍वर ज्योति हैं और उसमें कुछ भी अंधकार नहीं*।\"),\n",
    "(62001006,\"यदि हम कहें, कि उसके साथ हमारी सहभागिता है, और फिर अंधकार में चलें, तो हम झूठ बोलते है और सत्य पर नहीं चलते।\"),\n",
    "(62001007,\"पर यदि जैसा वह ज्योति में है, वैसे ही हम भी ज्योति में चलें, तो एक दूसरे से सहभागिता रखते हैं और उसके पुत्र यीशु मसीह का लहू हमें सब पापों से शुद्ध करता है। (यशा. 2:5)\"),\n",
    "(62001008,\"यदि हम कहें, कि हम में कुछ भी पाप नहीं, तो अपने आप को धोखा देते हैं और हम में सत्य नहीं।\"),\n",
    "(62001009,\"यदि हम अपने पापों को मान लें, तो वह हमारे पापों को क्षमा करने, और हमें सब अधर्म से शुद्ध करने में विश्वासयोग्य और धर्मी है। (भज. 32:5, नीति. 28:13)\"),\n",
    "(62001010,\"यदि हम कहें कि हमने पाप नहीं किया, तो उसे झूठा ठहराते हैं, और उसका वचन हम में नहीं है।\")]\n",
    "\n",
    "tokenize(\"hin\", sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-payday",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get Text functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-serbia",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How about we define a get text function on every content table(like bible, commentary etc), which would return the cleaned text field contents along with an id for that specific table?\n",
    "\n",
    "*Answer*: Yes. But design it as an abstract class which is inherited and implemented for each kind of sources\n",
    "\n",
    "* This list of sentences could be used as input for tokenization and draft generation. \n",
    "* Also this could be used for apps like Autographa or BridgeEngine to display reference texts on screen, as it would contain just the clean contents and no foot notes, cross-refs, strongs markups, alignments or any other non-relevant contents in USFM files\n",
    "* This would also come in handy for model building scripts to get the texts from varoius content tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-warren",
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import schemas, db_models\n",
    "import main\n",
    "\n",
    "def get_text_from_bible(db_, source_name, ref_start:schemas.Reference=None, \n",
    "    ref_end:schemas.Reference=None):\n",
    "    '''fetched text contents from bible_cleaned tables to be used for translations apps \n",
    "    or for model building.\n",
    "    Output format: [(id, sentance), (id, sentance), ....]'''\n",
    "    if source_name not in db_models.dynamicTables:\n",
    "        print(db_models.dynamicTables)\n",
    "        raise NotAvailableException('%s not found in database.'%source_name)\n",
    "    if not source_name.endswith('_bible'):\n",
    "        raise TypeException('The operation is supported only on bible')\n",
    "    model_cls = db_models.dynamicTables[source_name+'_cleaned']\n",
    "    ref_id_start = ref_id_end = None\n",
    "    if ref_start:\n",
    "    \tbook = db_models.BibleBook.filter(db_models.BibleBook.bookCode == ref_start.bookCode).first()\n",
    "    \tif not book:\n",
    "    \t\traise NotAvailableException(\"Book %s, not found in database\"%ref_start.bookCode)\n",
    "    \tref_id_start = book.bookId*1000000 + ref_start.chapter*1000 + ref_start.verseNumber\n",
    "    if ref_end:\n",
    "    \tbook = db_models.BibleBook.filter(db_models.BibleBook.bookCode == ref_end.bookCode).first()\n",
    "    \tif not book:\n",
    "    \t\traise NotAvailableException(\"Book %s, not found in database\"%ref_end.bookCode)\n",
    "    \tref_id_end = book.bookId*1000000 + ref_end.chapter*1000 + ref_end.verseNumber\n",
    "    if not ref_id_start:\n",
    "    \tref_id_start = 0\n",
    "    if not ref_id_end:\n",
    "    \tref_id_end = 999999999\n",
    "    query = db_.query(model_cls).filter(model_cls.refId >= ref_id_start,\n",
    "    \tmodel_cls.refId <= ref_id_end, model_cls.active == True)\n",
    "    res = query.all()\n",
    "    formatted_res = []\n",
    "    for item in res:\n",
    "        formatted_res.append((item.refId, item.verseText))\n",
    "    return formatted_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "environmental-plymouth",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(41001001, 'इब्राहीम की सन्\\u200dतान, दाऊद की ...'),\n",
       " (42001001, 'इब्राहीम की सन्\\u200dतान, दाऊद की ...'),\n",
       " (43001001, 'इब्राहीम की सन्\\u200dतान, दाऊद की ...')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from database import SessionLocal, engine\n",
    "from custom_exceptions import NotAvailableException, TypeException, AlreadyExistsException\n",
    "\n",
    "db_ = SessionLocal()\n",
    "\n",
    "get_text_from_bible(db_, source_name=\"hin_KJV_1_bible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-lesson",
   "metadata": {},
   "source": [
    "## Draft Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-booking",
   "metadata": {},
   "source": [
    "In V1, draft generation was done by find and replace of tokens(in the descending order of length of token) on the USFM file.\n",
    "\n",
    "In V2, as we are doing context based translation, it needs to be changed. We will be doing a replacement of tokens with translations on specific occurance.\n",
    "\n",
    "How about we do not use the input(source/reference USFM) for this replacement, instead create a fresh minimal USFM with the translated verses? For this we will be using the clean verse text we extracted from USFM and obtained using the the `get_text_from_bible()` function(the same text given for tokenizarion and displaying on UI), translate it using token replacement and then attach the minimum required markers \\id, \\c, \\p and \\v appropriately.\n",
    "\n",
    "By doing this, all non-verse contents present in the source/reference USFM would be absent in the generated draft. \n",
    "\n",
    "An existing issue in the draft of V1 is that some words are not replaced with translations even though, they are present in tokens list and translated there. I think the issue happens because they are part of phrase tokens and these phrases are broken apart in USFM file with additional markup in between them. So the find and replace doesnt work. Similar issues will occur for us in V2 also even if we are using offsets. So I think, using the cleaned text for replacement would be better\n",
    "\n",
    "*Answer*: Yes. It is better to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "after-israel",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def display_alignment(source, draft, draft_meta):\n",
    "    for item in draft_meta:\n",
    "        src = source[item[0][0]:item[0][1]]\n",
    "        trg = draft[item[1][0]:item[1][1]]\n",
    "        status = item[2]\n",
    "        print(src,\" --> \",trg,\"(\",status,\")\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "classical-grenada",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ഹലോ, my dear friend! [((0, 5), [0, 3], 'confirmed'), ((5, 22), (3, 20), 'untranslated')]\n",
      "\n",
      "\n",
      "hello  -->  ഹലോ ( confirmed )\n",
      ", my dear friend!  -->  , my dear friend! ( untranslated )\n",
      "---------------\n",
      "\n",
      "ഹലോ, my dear ചങ്ങാതീ! [((0, 5), [0, 3], 'confirmed'), ((5, 15), (3, 13), 'untranslated'), ((15, 21), [13, 20], 'suggestion'), ((21, 22), (20, 21), 'untranslated')]\n",
      "\n",
      "\n",
      "hello  -->  ഹലോ ( confirmed )\n",
      ", my dear   -->  , my dear  ( untranslated )\n",
      "friend  -->  ചങ്ങാതീ ( suggestion )\n",
      "!  -->  ! ( untranslated )\n",
      "---------------\n",
      "\n",
      "ഹലോ, എന്റെ dear ചങ്ങാതീ! [((0, 5), [0, 3], 'confirmed'), ((5, 7), (3, 5), 'untranslated'), ((7, 9), [5, 10], 'confirmed'), ((9, 15), (10, 16), 'untranslated'), ((15, 21), (16, 23), 'suggestion'), ((21, 22), (23, 24), 'untranslated')]\n",
      "\n",
      "\n",
      "hello  -->  ഹലോ ( confirmed )\n",
      ",   -->  ,  ( untranslated )\n",
      "my  -->  എന്റെ ( confirmed )\n",
      " dear   -->   dear  ( untranslated )\n",
      "friend  -->  ചങ്ങാതീ ( suggestion )\n",
      "!  -->  ! ( untranslated )\n",
      "---------------\n",
      "\n",
      "ഹലോ, എന്റെ പ്രിയ ചങ്ങാതീ! [((0, 5), [0, 3], 'confirmed'), ((5, 7), (3, 5), 'untranslated'), ((7, 14), [5, 16], 'confirmed'), ((14, 15), (16, 17), 'untranslated'), ((15, 21), (17, 24), 'suggestion'), ((21, 22), (24, 25), 'untranslated')]\n",
      "\n",
      "\n",
      "hello  -->  ഹലോ ( confirmed )\n",
      ",   -->  ,  ( untranslated )\n",
      "my dear  -->  എന്റെ പ്രിയ ( confirmed )\n",
      "   -->    ( untranslated )\n",
      "friend  -->  ചങ്ങാതീ ( suggestion )\n",
      "!  -->  ! ( untranslated )\n",
      "---------------\n",
      "\n",
      "ഹലോ, എന്റെ പ്രിയ സുഹൃത്തെ*** [((0, 5), [0, 3], 'confirmed'), ((5, 7), (3, 5), 'untranslated'), ((7, 14), [5, 16], 'confirmed'), ((14, 15), (16, 17), 'untranslated'), ((15, 22), [17, 28], 'confirmed')]\n",
      "\n",
      "\n",
      "hello  -->  ഹലോ ( confirmed )\n",
      ",   -->  ,  ( untranslated )\n",
      "my dear  -->  എന്റെ പ്രിയ ( confirmed )\n",
      "   -->    ( untranslated )\n",
      "friend!  -->  സുഹൃത്തെ*** ( confirmed )\n",
      "---------------\n",
      "\n",
      "ഹലോ, എന്റെ പ്രിയ സുഹൃത്തെ! [((0, 5), [0, 3], 'confirmed'), ((5, 7), (3, 5), 'untranslated'), ((7, 14), [5, 16], 'confirmed'), ((14, 15), (16, 17), 'untranslated'), ((15, 21), [17, 25], 'confirmed'), ((21, 22), (25, 26), 'untranslated')]\n",
      "\n",
      "\n",
      "hello  -->  ഹലോ ( confirmed )\n",
      ",   -->  ,  ( untranslated )\n",
      "my dear  -->  എന്റെ പ്രിയ ( confirmed )\n",
      "   -->    ( untranslated )\n",
      "friend  -->  സുഹൃത്തെ ( confirmed )\n",
      "!  -->  ! ( untranslated )\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "def replace_token(source, token_offset, translation, draft=\"\", draft_meta=[], tag=\"confirmed\"):\n",
    "    '''make a token replacement and return updated sentence and draft_meta'''\n",
    "#     pdb.set_trace()\n",
    "    token_length = token_offset[1] - token_offset[0]\n",
    "    trans_length = len(translation)\n",
    "    updated_meta = []\n",
    "    updated_draft = \"\"\n",
    "    translation_offset = [None, None]\n",
    "    if len(draft_meta) == 0:\n",
    "        draft = source\n",
    "        draft_meta.append(((0,len(source)), (0,len(source)), \"untranslated\"))\n",
    "    for meta in draft_meta:\n",
    "        tkn_offset = meta[0]\n",
    "        trans_offset = meta[1]\n",
    "        status = meta[2]\n",
    "        intersection = set(range(token_offset[0],token_offset[1])).intersection(range(tkn_offset[0],tkn_offset[1]))\n",
    "        if len(intersection) > 0: # our area of interest overlaps with this segment \n",
    "            if token_offset[0] == tkn_offset[0]: #begining is same\n",
    "                translation_offset[0] = trans_offset[0]\n",
    "                updated_draft += translation\n",
    "            elif token_offset[0] > tkn_offset[0]: # begins within this segment\n",
    "                updated_draft += source[tkn_offset[0]: token_offset[0]]\n",
    "                new_seg_len = token_offset[0] - tkn_offset[0]\n",
    "                updated_meta.append(((tkn_offset[0], token_offset[0]), (trans_offset[0], trans_offset[0]+new_seg_len),\"untranslated\"))\n",
    "                translation_offset[0] = trans_offset[0]+new_seg_len\n",
    "                updated_draft += translation\n",
    "            else: # begins before this segment\n",
    "                pass\n",
    "            if token_offset[1] == tkn_offset[1]: # ending is the same\n",
    "                translation_offset[1] = translation_offset[0]+trans_length\n",
    "                updated_meta.append((token_offset, translation_offset, tag))\n",
    "                offset_diff = translation_offset[1] - trans_offset[1]\n",
    "            elif token_offset[1] < tkn_offset[1]: # ends within this segment\n",
    "                trailing_seg = source[token_offset[1]: tkn_offset[1]]\n",
    "                translation_offset[1] = translation_offset[0]+trans_length\n",
    "                updated_meta.append((token_offset, translation_offset, tag))\n",
    "                updated_draft += trailing_seg\n",
    "                updated_meta.append(((token_offset[1], tkn_offset[1]),(translation_offset[1],translation_offset[1]+len(trailing_seg)),\"untranslated\"))\n",
    "                offset_diff = translation_offset[1]+len(trailing_seg) - trans_offset[1]\n",
    "            else: # ends after this segment\n",
    "                pass\n",
    "        elif tkn_offset[1] < token_offset[1]: # our area of interest come after this segment\n",
    "            updated_draft += draft[trans_offset[0]: trans_offset[1]]\n",
    "            updated_meta.append(meta)\n",
    "        else: # our area of interest was before this segment\n",
    "            updated_draft += draft[trans_offset[0]: trans_offset[1]]\n",
    "            updated_meta.append((tkn_offset, (trans_offset[0]+offset_diff, trans_offset[1]+offset_diff),status))\n",
    "    return updated_draft, updated_meta\n",
    "    \n",
    "source = \"hello, my dear friend!\"\n",
    "    \n",
    "draft, meta = replace_token(source, (0,5), \"ഹലോ\")\n",
    "print(draft, meta)\n",
    "print('\\n')\n",
    "display_alignment(source, draft, meta)\n",
    "print(\"---------------\\n\")\n",
    "draft, meta = replace_token(source, (15,21), \"ചങ്ങാതീ\", draft, meta, \"suggestion\")\n",
    "print(draft, meta)\n",
    "print('\\n')\n",
    "display_alignment(source, draft, meta)\n",
    "print(\"---------------\\n\")\n",
    "draft, meta = replace_token(source, (7,9), \"എന്റെ\", draft, meta)\n",
    "print(draft, meta)\n",
    "print('\\n')\n",
    "display_alignment(source, draft, meta)\n",
    "print(\"---------------\\n\")\n",
    "draft, meta = replace_token(source, (7,14), \"എന്റെ പ്രിയ\", draft, meta)\n",
    "print(draft, meta)\n",
    "print('\\n')\n",
    "display_alignment(source, draft, meta)\n",
    "print(\"---------------\\n\")\n",
    "draft, meta = replace_token(source, (15,22), \"സുഹൃത്തെ***\", draft, meta)\n",
    "print(draft, meta)\n",
    "print('\\n')\n",
    "display_alignment(source, draft, meta)\n",
    "print(\"---------------\\n\")\n",
    "draft, meta = replace_token(source, (15,21), \"സുഹൃത്തെ\", draft, meta)\n",
    "print(draft, meta)\n",
    "print('\\n')\n",
    "display_alignment(source, draft, meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-tokyo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "geological-blood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, ['do', 'the', 'work', 'and', 'then', 'come'])\n",
      "(2, ['I', 'do', 'the work', 'and', 'then', 'come'])\n",
      "(2, ['I', 'do', 'the work', 'and', 'then', 'come'])\n",
      "(0, ['work', 'and', 'then', 'come'])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from math import floor, ceil\n",
    "def extract_context(token, offset, sentence, window_size=5, punctuations=utils.punctuations()+utils.numbers()):\n",
    "    '''return token index and context array'''\n",
    "    punct_pattern = re.compile('['+''.join(punctuations)+']')\n",
    "    front = sentence[:offset[0]]\n",
    "    rear = sentence[offset[1]:]\n",
    "    front = re.sub(punct_pattern, \"\", front)\n",
    "    rear = re.sub(punct_pattern, \"\", rear)\n",
    "    front = front.split()\n",
    "    rear = rear.split()\n",
    "    if len(front) >= window_size/2:\n",
    "        front  = front[-floor(window_size/2):]\n",
    "    if len(rear) >= window_size/2:\n",
    "        rear = rear[:ceil(window_size/2)]\n",
    "    index = len(front)\n",
    "    context = front + [token] + rear\n",
    "    return index, context\n",
    "\n",
    "print(extract_context(\"work\", (14,18), \"better do the work and then come back to check\"))\n",
    "print(extract_context(\"the work\", (5,13), \"I do the work and then come back to check\"))\n",
    "print(extract_context(\"the work\", (5,13), \"I do the work and, then come back to check\"))\n",
    "print(extract_context(\"work\", (0,4), \"work and then come back to check\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "worthy-rebate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{41001001: {'source': \"Here is it, a sample: it tells us what? yes a lot!!! isn't it\",\n",
       "  'draft': \"Here is it, a sample: it tells us what? yes a lot!!! isn't it\",\n",
       "  'draft_meta': [((0, 61), (0, 61), 'untranslated')]},\n",
       " 41001002: {'source': 'Once there was a hare and a tortoise.',\n",
       "  'draft': 'Once there was a മുയല്\\u200d and a tortoise.',\n",
       "  'draft_meta': [((0, 17), (0, 17), 'untranslated'),\n",
       "   ((17, 21), [17, 23], 'suggestion'),\n",
       "   ((21, 37), (23, 39), 'untranslated')]},\n",
       " 41001003: {'source': 'One day, they got into an argument.',\n",
       "  'draft': 'One day, they got into an argument.',\n",
       "  'draft_meta': [((0, 35), (0, 35), 'untranslated')]},\n",
       " 41001004: {'source': \"The hare said, 'I am the fastest'.\",\n",
       "  'draft': \"The മുയല്\\u200d said, 'I am the fastest'.\",\n",
       "  'draft_meta': [((0, 4), (0, 4), 'untranslated'),\n",
       "   ((4, 8), [4, 10], 'suggestion'),\n",
       "   ((8, 34), (10, 36), 'untranslated')]},\n",
       " 41001005: {'source': \"Then tortoise replied, 'no no no'\",\n",
       "  'draft': \"Then tortoise replied, 'no no no'\",\n",
       "  'draft_meta': [((0, 33), (0, 33), 'untranslated')]}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pdb\n",
    "import utils\n",
    "   \n",
    "def auto_translate(sentence_list, source_lang, target_lang,\n",
    "    punctuations=None, stop_words=None):\n",
    "    '''Attempts to tokenize the input sentence and replace each token with top suggestion.\n",
    "    If draft_meta is provided indicating some portion of sentence is user translated, it is left untouched.\n",
    "    Output is of the format [(sent_id, translated text, metadata)]\n",
    "    metadata: List of (start_offset, end_offset, confirmed/suggestion/untranslated)'''\n",
    "    if not punctuations:\n",
    "        punctuations = utils.punctuations()+utils.numbers()\n",
    "        punct_pattern = re.compile('['+''.join(punctuations)+']')\n",
    "    if not stop_words:\n",
    "        stop_words = utils.stopwords(source_lang)\n",
    "    sentence_dict = {}\n",
    "    for item in sentence_list:\n",
    "        sent_obj = {\n",
    "            \"source\": item[1],\n",
    "#             \"source_chunks\": [chunk.strip() for chunk in re.split(punct_pattern, item[1])],\n",
    "            \"draft\":item[2],\n",
    "            \"draft_meta\":item[3]\n",
    "        }\n",
    "        if sent_obj['draft'] is None:\n",
    "            sent_obj['draft'] = ''\n",
    "        if sent_obj['draft_meta'] is None:\n",
    "            sent_obj['draft_meta'] = []\n",
    "        sentence_dict[item[0]]=sent_obj \n",
    "    suggestions_model = t # load corresponding trie for source and target if not already in memory\n",
    "    tokens = tokenize(source_lang, sentence_list)\n",
    "    for token in tokens:\n",
    "        for occurence in tokens[token]:\n",
    "            offset = (occurence[1], occurence[2])\n",
    "            index, context = extract_context(token, offset, sentence_dict[occurence[0]]['source'])\n",
    "            suggestions = get_translation_suggestion(index, context, t)\n",
    "            if len(suggestions) > 0:\n",
    "                draft, meta = replace_token(sentence_dict[occurence[0]]['source'], offset, suggestions[0][0], \n",
    "                              sentence_dict[occurence[0]]['draft'], sentence_dict[occurence[0]]['draft_meta'], \n",
    "                              \"suggestion\")\n",
    "                sentence_dict[occurence[0]]['draft'] = draft\n",
    "                sentence_dict[occurence[0]]['draft_meta'] = meta\n",
    "            elif sentence_dict[occurence[0]]['draft'] == '':\n",
    "                sentence_dict[occurence[0]]['draft'] = sentence_dict[occurence[0]]['source']\n",
    "                indices = (0,len(sentence_dict[occurence[0]]['source']))\n",
    "                sentence_dict[occurence[0]]['draft_meta'] = [(indices, indices, \"untranslated\")]\n",
    "                \n",
    "    return sentence_dict\n",
    "\n",
    "sentences = [(41001001, \"Here is it, a sample: it tells us what? yes a lot!!! isn't it\", None, None),\n",
    "             (41001002, \"Once there was a hare and a tortoise.\", None, None),\n",
    "             (41001003, \"One day, they got into an argument.\", None, None),\n",
    "             (41001004, \"The hare said, 'I am the fastest'.\", None, None),\n",
    "             (41001005, \"Then tortoise replied, 'no no no'\", None, None)]\n",
    "auto_translate(sentences, \"eng\", \"mal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_draft(db_, project_id, sent_ids=None):\n",
    "    '''fetches drafts for a set of sentences in a project, to be displayed on UI or sent for USFM creation'''\n",
    "    project = db_.query(AgmtProject).filter(db_models.AgmtProject.projectId == project_id).first()\n",
    "    if project is None:\n",
    "        raise NotAvailableException(\"The given project not available in database\")\n",
    "    \n",
    "    query = db_.query(db_models.TranslationDrafts).filter(\n",
    "        db_models.TranslationDrafts.project_id == project_id)\n",
    "    if sent_ids:\n",
    "        query = query.filter(db_models.TranslationDrafts..sentenceId in sent_ids)\n",
    "    return drafts    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "heard-packet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\id GEN\\n\\\\c 1\\n\\\\p\\n\\\\v 1 the first verse\\\\v 2 the second verse\\\\v 3 the verse out of order\\\\c 2\\n\\\\p\\n\\\\v 1 the first verse of new chapter\\\\v 2 the next verse\\\\v 3 the last verse',\n",
       " '\\\\id EXO\\n\\\\c 1\\n\\\\p\\n\\\\v 1 the first verse of new book\\\\v 1 the next verse']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdb\n",
    "import utils\n",
    "from custom_exceptions import NotAvailableException\n",
    "\n",
    "def create_usfm(sent_drafts):\n",
    "    '''Creates minimal USFM file with basic markers from the input verses list\n",
    "    input: List of (bbbcccvvv, \"generated translation\")\n",
    "    output: List of usfm files, one file per bible book'''\n",
    "    book_start = '\\\\id {}\\n'\n",
    "    chapter_start = '\\\\c {}\\n\\\\p\\n'\n",
    "    verse = '\\\\v {} {}'\n",
    "    usfm_files = []\n",
    "    file = ''\n",
    "    prev_book = 0\n",
    "    prev_chapter = 0\n",
    "    prev_verse = 0\n",
    "    book_code = ''\n",
    "    sentences = sorted(sent_drafts, key=lambda x:x[0], reverse=False)\n",
    "    #pdb.set_trace()\n",
    "    for sent in sentences:\n",
    "        #pdb.set_trace()\n",
    "        verse_num = sent[0] % 1000\n",
    "        chapter_num = int((sent[0] /1000) % 1000)\n",
    "        book_num = int(sent[0] / 1000000)\n",
    "        if book_num != prev_book:\n",
    "            if file != '':\n",
    "                usfm_files.append(file)\n",
    "            book_code = utils.book_code(book_num)\n",
    "            if book_code is None:\n",
    "                #pdb.set_trace()\n",
    "                raise NotAvailableException(\"Book number %s not a valid one\" %book_num)\n",
    "            file = book_start.format(book_code)\n",
    "            prev_book = book_num\n",
    "        if chapter_num != prev_chapter:\n",
    "            file += chapter_start.format(chapter_num)\n",
    "            prev_chapter = chapter_num\n",
    "        file += verse.format(verse_num, sent[1])\n",
    "    if file != '':\n",
    "        usfm_files.append(file)\n",
    "    return usfm_files\n",
    "\n",
    "draft = [\n",
    "    (1001001,\"the first verse\",[(0,2,\"confirmed\"), (4,13,\"suggestion\")]), #with metadata\n",
    "    (1001002,\"the second verse\"),\n",
    "    (1002001,\"the first verse of new chapter\"),\n",
    "    (1001003,\"the verse out of order\"), \n",
    "    (1002002,\"the next verse\"),\n",
    "    (1002003,\"the last verse\"),\n",
    "    (2001001,\"the first verse of new book\"),\n",
    "    (2001001,\"the next verse\"),\n",
    "    #(40001001, \"invalid book\"), \n",
    "]\n",
    "create_usfm(draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-parcel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "genetic-regular",
   "metadata": {},
   "source": [
    "## Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-logistics",
   "metadata": {},
   "source": [
    "For every language pair for which we have translation projects or previously available parallel aligned data, we will have a *translation memory learned model*. When we encounter a word in a particular context, we use this learned model to get all possible translations of the word and get them scored based on the current context window. These scored translation can be given to user as suggestions.\n",
    "\n",
    "I think the key thing to decide in this is, how do we store the translation memory to be used efficiently.\n",
    "Or what do we mean by a learned translation model for a language pair?\n",
    "\n",
    "The options that occur to me are as follows:\n",
    "1. Query the translation memory(alignment) **SQL table** based on the key word and check and sort them based on the context window(I am afraid, this will have poor performace in terms of time and space)\n",
    "2. Periodically build a **trie** structure from translation memory(alignment) table and query this trie for suggestions. I am not yet familiar with trie. I hope it allows us to search based on a context window efficiently. One draw back I can see in this is \"learning\" will not happen in real time and data user adds will take time(depending on how often we run the learning script) to improve the suggestions quality.\n",
    "3. While we keep a translation memory table in SQL DB, parallely bulid a **graph** structure with it in DGraph. Use this graph for suggestions and use the table in SQL DB for draft generation. \n",
    "4. Build a **nueral network** (or ML) model that can be trained with word and context window and can predict the translation. I am not sure if we can get such a model to give multiple translations with varing scores. Building, storing, and using such models can also become expensive in terms of time and space. \n",
    "\n",
    "*Answer*: Option 2, trie built from SQL table, for now and 3, 4 for later\n",
    "\n",
    "#### Proposed tire structure for AgMT\n",
    "\n",
    "* Have one trie per source-target language pair, this would reduce the size and thus increase search performance at level 1\n",
    "* Each node will have\n",
    "\t* a key: the context. The window size increases by one at each level\n",
    "\t* translations: list of all seen translations and their count for the given context. The count and current level can be used for scoring suggestions(score = level*count/total_occurances, total_occurance=sumOfCountsAtLevel1)\n",
    "\t* children: context increases by one word to right or left from the current context\n",
    "\n",
    "input: \n",
    "```\n",
    "[\n",
    "{\"token\": \"house\",\"context\":\"They use barrels to house their pets\",\"translation\":\"പാര്‍പ്പിക്കുക\"},\n",
    "{\"token\": \"house\",\"context\":\"His house is to the left\",\"translation\":\"വീട്\"},\n",
    "{\"token\": \"house\",\"context\":\"Their house contruction methods are different\",\"translation\":\"ഭവന\"},\n",
    "{\"token\": \"house\",\"context\":\"Last time I went to his house,\",\"translation\":\"വീട്ടിലേക്ക്\"},\n",
    "{\"token\": \"house\",\"context\":\"Museums house large collection of Roman sculpture\",\"translation\":\"ഉള്‍ക്കൊള്ളുന്നു\"}\n",
    "]\n",
    "```\n",
    "A trie of window size 3\n",
    "![trie diagram](example_trie.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "relevant-batch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house', 'house/L:his', 'house/R:is', 'house/L:his/R:is', 'house/R:is/L:his']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def form_trie_keys(prefix, to_left, to_right, prev_keys):\n",
    "    '''build the trie tree recursively'''    \n",
    "    keys = prev_keys\n",
    "    a = b = None\n",
    "    if len(to_left) > 0:\n",
    "        a = '/L:'+to_left.pop(0)\n",
    "    if len(to_right) > 0:\n",
    "        b = '/R:'+to_right.pop(0)\n",
    "    if a:\n",
    "        key_left = prefix + a\n",
    "        keys.append(key_left)\n",
    "        keys = form_trie_keys(key_left, to_left.copy(), to_right.copy(), keys)\n",
    "    if b:\n",
    "        key_right = prefix + b\n",
    "        keys.append(key_right)\n",
    "        keys = form_trie_keys(key_right, to_left.copy(), to_right.copy(), keys)\n",
    "    if a and b:\n",
    "        key_both_1 = prefix + a + b\n",
    "        key_both_2 = prefix + b + a\n",
    "        keys.append(key_both_1)\n",
    "        keys.append(key_both_2)\n",
    "        keys = form_trie_keys(key_both_1, to_left.copy(), to_right.copy(), keys)\n",
    "        keys = form_trie_keys(key_both_2, to_left.copy(), to_right.copy(), keys)\n",
    "    return keys\n",
    "\n",
    "token = \"house\"\n",
    "#context = [\"house\"]\n",
    "context = [\"his\", \"house\", \"is\"]\n",
    "#context = [\"his\", \"house\"]\n",
    "#context = [\"house\", \"is\"]\n",
    "#context = [\"says\",\"his\", \"house\", \"is\", \"in\", \"town\"]\n",
    "#context = [\"house\", \"is\", \"in\", \"town\"]\n",
    "#context = [\"he\",\"says\",\"his\", \"house\"]\n",
    "#context = [\"he\",\"says\",\"his\", \"house\", \"is\", \"in\", \"town\"]\n",
    "\n",
    "token_index = context.index(token)\n",
    "to_left = [context[i] for i in range(token_index-1, -1, -1)]\n",
    "to_right = context[token_index+1:]\n",
    "form_trie_keys(token, to_left, to_right, [token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-active",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "local-atlas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- bank =>  {'ബാങ്ക്': 3, 'ആശ്രയിക്കുക': 1, 'തീരം': 2}\n",
      "\t- bank\t- R:is =>  {'ബാങ്ക്': 1, 'തീരം': 1}\n",
      "\t- bank\t- R:is\t- R:closed =>  {'ബാങ്ക്': 1}\n",
      "\t- bank\t- R:is\t- R:muddy =>  {'തീരം': 1}\n",
      "\t- bank\t- R:is\t- L:river =>  {'തീരം': 1}\n",
      "\t- bank\t- R:is\t- L:river\t- R:muddy =>  {'തീരം': 1}\n",
      "\t- bank\t- L:they =>  {'ആശ്രയിക്കുക': 1}\n",
      "\t- bank\t- L:they\t- R:us =>  {'ആശ്രയിക്കുക': 1}\n",
      "\t- bank\t- L:they\t- R:on =>  {'ആശ്രയിക്കുക': 1}\n",
      "\t- bank\t- L:they\t- R:on\t- R:us =>  {'ആശ്രയിക്കുക': 1}\n",
      "\t- bank\t- R:on =>  {'ആശ്രയിക്കുക': 1, 'തീരം': 1}\n",
      "\t- bank\t- R:on\t- R:us =>  {'ആശ്രയിക്കുക': 1}\n",
      "\t- bank\t- R:on\t- L:they =>  {'ആശ്രയിക്കുക': 1}\n",
      "\t- bank\t- R:on\t- L:they\t- R:us =>  {'ആശ്രയിക്കുക': 1}\n",
      "\t- bank\t- R:on\t- L:has =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:has\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:has\t- R:sides =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:has\t- R:sides\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- R:sides =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- R:sides\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- R:sides\t- L:has =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- R:sides\t- L:has\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:wide =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:wide\t- L:has =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:wide\t- L:has\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:wide\t- L:has\t- R:sides =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:wide\t- L:has\t- R:sides\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:wide\t- R:sides =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:wide\t- R:sides\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:wide\t- R:sides\t- L:has =>  {'തീരം': 1}\n",
      "\t- bank\t- R:on\t- L:wide\t- R:sides\t- L:has\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- L:pay =>  {'ബാങ്ക്': 1}\n",
      "\t- bank\t- L:pay\t- R:back =>  {'ബാങ്ക്': 1}\n",
      "\t- bank\t- R:back =>  {'ബാങ്ക്': 1}\n",
      "\t- bank\t- R:back\t- L:pay =>  {'ബാങ്ക്': 1}\n",
      "\t- bank\t- L:river =>  {'തീരം': 1}\n",
      "\t- bank\t- L:river\t- R:muddy =>  {'തീരം': 1}\n",
      "\t- bank\t- L:river\t- R:is =>  {'തീരം': 1}\n",
      "\t- bank\t- L:river\t- R:is\t- R:muddy =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- L:has =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- L:has\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- L:has\t- R:sides =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- L:has\t- R:sides\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:sides =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:sides\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:sides\t- L:has =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:sides\t- L:has\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:on =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:on\t- L:has =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:on\t- L:has\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:on\t- L:has\t- R:sides =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:on\t- L:has\t- R:sides\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:on\t- R:sides =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:on\t- R:sides\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:on\t- R:sides\t- L:has =>  {'തീരം': 1}\n",
      "\t- bank\t- L:wide\t- R:on\t- R:sides\t- L:has\t- L:Ganga =>  {'തീരം': 1}\n",
      "\t- bank\t- R:manager =>  {'ബാങ്ക്': 1}\n",
      "\t- bank\t- R:manager\t- R:spoke =>  {'ബാങ്ക്': 1}\n",
      "\t- hare =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- L:a =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- L:a\t- R:same =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- L:a\t- R:same\t- R:as =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- L:a\t- R:same\t- R:as\t- R:rabbit =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- L:a\t- R:is =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- L:a\t- R:is\t- R:same =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- L:a\t- R:is\t- R:same\t- R:as =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- L:a\t- R:is\t- R:same\t- R:as\t- R:rabbit =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- R:is =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- R:is\t- R:same =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- R:is\t- R:same\t- R:as =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- R:is\t- R:same\t- R:as\t- R:rabbit =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- R:is\t- L:a =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- R:is\t- L:a\t- R:same =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- R:is\t- L:a\t- R:same\t- R:as =>  {'മുയല്\\u200d': 1}\n",
      "\t- hare\t- R:is\t- L:a\t- R:same\t- R:as\t- R:rabbit =>  {'മുയല്\\u200d': 1}\n"
     ]
    }
   ],
   "source": [
    "import pygtrie\n",
    "from custom_exceptions import TypeException\n",
    "\n",
    "def build_trie(token_context__trans_list):\n",
    "    '''Build a trie tree from scratch\n",
    "    input: [(token,context_list, translation), ...]'''\n",
    "    t = pygtrie.StringTrie()\n",
    "    for item in token_context__trans_list:\n",
    "        context = item[1]\n",
    "        translation = item[2]\n",
    "        if isinstance(item[0], str):\n",
    "            token = item[0]\n",
    "            token_index = context.index(token)\n",
    "        elif isinstance(item[0], int):\n",
    "            token_index = item[0]\n",
    "            token = context[token_index]\n",
    "        else:\n",
    "            raise TypeException(\"Expects the token, as string, or index of token, as int, in first field of input tuple\")\n",
    "        to_left = [context[i] for i in range(token_index-1, -1, -1)]\n",
    "        to_right = context[token_index+1:]\n",
    "        keys = form_trie_keys(token, to_left, to_right, [token])\n",
    "        for key in keys:\n",
    "            if t.has_key(key):\n",
    "                value = t[key]\n",
    "                if translation in value.keys():\n",
    "                    value[translation] += 1\n",
    "                else:\n",
    "                    value[translation] = 1\n",
    "                t[key] = value\n",
    "            else:\n",
    "                t[key] = {translation: 1}\n",
    "    return t\n",
    "\n",
    "training_data = [\n",
    "    (\"bank\", [\"bank\", \"is\", \"closed\"], \"ബാങ്ക്\"),\n",
    "    (\"bank\", [\"they\", \"bank\", \"on\", \"us\"], \"ആശ്രയിക്കുക\"),\n",
    "    (\"bank\", [\"pay\", \"bank\", \"back\"], \"ബാങ്ക്\"),\n",
    "    (\"bank\", [\"river\", \"bank\", \"is\", \"muddy\"], \"തീരം\"),\n",
    "    (\"bank\", [\"Ganga\",\"has\", \"wide\" , \"bank\", \"on\", \"sides\"], \"തീരം\"),\n",
    "    (\"bank\", [\"bank\", \"manager\", \"spoke\"], \"ബാങ്ക്\"),\n",
    "    (\"hare\", [\"a\", \"hare\", \"is\", \"same\", \"as\", \"rabbit\"], \"മുയല്‍\")]\n",
    "t = build_trie(training_data)\n",
    "\n",
    "\n",
    "display_tree(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-master",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "metropolitan-liverpool",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('തീരം', 1.5),\n",
       " ('ബാങ്ക്', 0.6666666666666666),\n",
       " ('ആശ്രയിക്കുക', 0.16666666666666666)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdb;\n",
    "\n",
    "def get_translation_suggestion(word, context, t):\n",
    "    '''find the context based translation suggestions for a word.\n",
    "    Makes use of the learned model, t, for the lang pair, based on translation memory\n",
    "    output format: [(translation1, score1), (translation2, score2), ...]'''\n",
    "    if isinstance(word, str):\n",
    "        token_index = context.index(word)\n",
    "    elif isinstance(word, int):\n",
    "        token_index = word\n",
    "        word = context[token_index]\n",
    "    single_word_match = list(t.prefixes(word))\n",
    "    #pdb.set_trace()\n",
    "    if len(single_word_match) == 0:\n",
    "        return []\n",
    "    total_count = sum(single_word_match[0].value.values())\n",
    "    to_left = [context[i] for i in range(token_index-1, -1, -1)]\n",
    "    to_right = context[token_index+1:]\n",
    "    keys = form_trie_keys(word, to_left, to_right, [word])\n",
    "    keys = sorted(keys, key = lambda x : len(x), reverse=True)\n",
    "    suggestions = {}\n",
    "    prev_path_length = 0\n",
    "    for k in keys:\n",
    "        if len(k) < prev_path_length:\n",
    "            # avoid searching with all the lower level keys\n",
    "            break\n",
    "        prev_path_length = len(k)\n",
    "        all_matches = t.prefixes(k)\n",
    "        for match in all_matches:\n",
    "            levels = len(match.key.split(\"/\"))\n",
    "            #pdb.set_trace()\n",
    "            for trans in match.value:\n",
    "                score = match.value[trans]*levels*levels / total_count\n",
    "                if trans in suggestions:\n",
    "                    if suggestions[trans] < score:\n",
    "                        suggestions[trans] = score\n",
    "                else:\n",
    "                    suggestions[trans] = score\n",
    "    sorted_suggestions = {k: suggestions[k] for k in sorted(suggestions, key=suggestions.get, reverse=True)}\n",
    "    return [(key, suggestions[key]) for key in sorted_suggestions]\n",
    "\n",
    "#get_translation_suggestion(\"bank\", [\"pay\", \"bank\", \"the\", \"money\"], t)\n",
    "#get_translation_suggestion(\"bank\", [\"bank\", \"the\", \"money\"], t)\n",
    "get_translation_suggestion(1, [\"river\", \"bank\", \"is\", \"near\"], t)\n",
    "#get_translation_suggestion(\"bank\", [\"people\", \"bank\", \"on\", \"others\"], t)\n",
    "#get_translation_suggestion(\"bank\", [\"bank\"], t)\n",
    "#get_translation_suggestion(\"hill\", [\"that\",\"hill\", \"is\"], t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-competition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "surgical-european",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-sellers",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Projects and Translation memory tables](agmt_tables.png)\n",
    "\n",
    "### Changes to above design\n",
    "\n",
    "* Do not store suggestions in DB. Obtain them from trie dynamically (removes it from translation memory). Or may be we can store it in the drafts/draft_metadata field, by keeping top 3 suggestions there, and not just the top one.\n",
    "* Move the source sentences and drafts from projects table to a separate table(drafts)\n",
    "* Store sentence/verse wise drafts in drafts table along with which part of that draft has\n",
    "    * user confirmed translation\n",
    "    * top scored suggestions\n",
    "    * not translated\n",
    "    \n",
    "    in metadata\n",
    "    eg: `[((0, 5), (0, 3), 'confirmed'), ((5, 15), (3, 13), 'untranslated'), ((15, 21), (13, 20), 'suggestion')]`\n",
    "    \n",
    "    This will allow UI to display them with different styles. Also allow us to have phrase tokens and convert the data into the form of an alignment JSON for data export\n",
    "* Also store who corrected each verse/sentence last. The SB alignment flavor has to specify which user did which reference range. Also adds user info to translation memory table\n",
    "* split earlier composite fields into separate columns(occurences and sentences)\n",
    "* The `translation memory` table is made independant of projects. \n",
    "* user table is removed, as that data will be stored in Kratos    \n",
    "\n",
    "![Revised tables](./agmt_tables3.png)\n",
    "\n",
    "### How these table structures work\n",
    "\n",
    "* **Projects table**: For every AgMT project user starts, there will be one entry here. The document type will indicate the output format, which will be USFM for the current scope of AgMT mode(ie., only bible translation)\n",
    "* **Drafts table**: For all the AgMT projects their **entire source and coressponding draft**(according to current project status) will be saved here. Each row will have one sentence/verse. For a project that uses all/some books from an exiting bible in DB, these source sentences will be copied from the corresponding `_bible_cleaned` table. If the user uploads hisown usfm, the verse texts from that will be copied here. This will allow user to use their own sources/refernce bibles. Also user can add more books to a existing project.\n",
    "    * `sentence_id`: For a bible translation project this will be `ref_id`(bbbcccvvv). For a Notes or commentary project, we want to include later, we can use this `ref_id` itself. For a story project, may be paragraghCount-sentenceCount can be used. The idea is, we should be able to generate the draft in a required format as USFM, CSV, doc etc from this numbering system for that specific documentType/projectType. `sentence_id` and `project_id` combo will be unique and indexed. The table will be searched based on these fields for retrieving all occurences of a token, for displaying those sentences/verses and drafts on UI.\n",
    "    * `sentence`: will have the actual source sentence, the verse, for a bible translation project. when user is working on a token, we can display all sentences/verses where it occurs on screen, so he can choose different senses for each occurance, if applicable.\n",
    "    * `draft`: will have the current generated draft, for the corresponding sentence/verse. This may include some user-translated tokens, some untranslated tokens and some tokens replaced by top suggestions we provide. This field, along with the next `draft_meta` field will be where the project data will be updated as the translation progresses. Saving the draft like this will allow to display them on screen in real time, for all the sentence/verses where the current token occurs, allowing user to make proper decisions on which sense to be used in the context and also get a better sense of translation progress. This field will a plain text field. For the USFM generation we will be taking this field and generating the USFM based on the `sentence_id` values.\n",
    "    * `draft_meta`: This will a JSON field of the following format. `[((0, 5), (0, 3), 'confirmed'), ((5, 15), (3, 13), 'untranslated'), ((15, 21), (13, 20), 'suggestion')]`. It is here the source sentence is split up into segments by start and end offsets and indicate the status of how the draft is obtained for each segment. When user makes a translation those tokens will be replaced in the draft and corresponding change will be made here setting that source token's offset values to \"confirmed\". Similary when we run the suggestions module on the entire project or specific sentences, we would replace tokens in drafts with top suggestions and update those offsets and values to \"suggestion\" here. This info can be used to display the generated draft with different styles on the UI, for example a dark colour for confirmed translations, a moderated colour for suggestions and lighter shade for un-translated. Thus user will have a clear idea on progress of work upon seeing the drafts displayed.\n",
    "* **translation_memory table**: This stores all the known tokens and their known translations.\n",
    "    * updation: Normally it is updated, along with the draft table, when ever user makes a translation on the AgMT UI and saves it. If we are able to add an alignmnet mode to the tool which allows to user to change alginement smanually, that data will also be updated here. It can also be updated externally if we have aligned training data from else where.\n",
    "    * accesing: This table can be refered to check if a token is known to us, while tokenization. Also it can be used to get all known translations of a token irrespective of context. The source_lang, target_lang, token combo is unique and indexed.\n",
    "* One info missing, with this kind of table structure, would be, who made a specific translation, as we only store last upated user for the whole sentence/verse\n",
    "* The tokens generated are not stored anywhere in DB, unless they have a user confirmed translation. This allows us to keep tokenization dynamic and based on tokens marked by user via alignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-capacity",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-trance",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
